<!doctype html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <title>WebGPU Subgroup Matrix</title>
</head>

<body>
    <pre id="output"></pre>

    <script>
        function log(...args) {
            document.getElementById("output").textContent += args.join(" ") + "\n";
        }

        function printMatrix(A, rows, cols, verbose = false, padding = null) {
            // Helper to pad a value to a string of given width
            function padValue(val, width) {
                let s = String(val);
                if (width == null) return s;
                // If val is a number, format to fixed decimals for alignment
                if (typeof val === "number") {
                    // Try to keep 4 significant digits for floats
                    if (Number.isInteger(val)) {
                        s = val.toString();
                    } else {
                        s = val.toFixed(3);
                    }
                }
                // If it's an ellipsis, just center it
                if (s === "...") {
                    let left = Math.floor((width - 3) / 2);
                    let right = width - 3 - left;
                    return " ".repeat(left) + "..." + " ".repeat(right);
                }
                // Pad left for numbers, right for others
                return s.padStart(width, " ");
            }

            if (verbose) {
                // Print full matrix
                for (let i = 0; i < rows; i++) {
                    let row = [];
                    for (let j = 0; j < cols; j++) {
                        row.push(padValue(A[i * cols + j], padding));
                    }
                    log(row.join(" "));
                }
            } else {
                // Print abbreviated matrix with ellipsis
                for (let i = 0; i < rows; i++) {
                    if (i < 3 || i > rows - 4) {
                        let row = [];
                        for (let j = 0; j < cols; j++) {
                            if (j < 3 || j > cols - 4) {
                                row.push(padValue(A[i * cols + j], padding));
                            } else if (j === 3) {
                                row.push(padValue("...", padding));
                            }
                        }
                        log(row.join(" "));
                    } else if (i === 3) {
                        // Print a row of ellipsis, padded if needed
                        let ellipsisRow = [];
                        for (let j = 0; j < cols; j++) {
                            if (j < 3 || j > cols - 4) {
                                ellipsisRow.push(padValue("...", padding));
                            } else if (j === 3) {
                                ellipsisRow.push(padValue("...", padding));
                            }
                        }
                        log(ellipsisRow.join(" "));
                    }
                }
            }
        }


        // Problem size, matrix
        const n_VALUE = 2024;
        const A_LEN = n_VALUE * n_VALUE;
        const B_LEN = n_VALUE * n_VALUE;
        const C_LEN = n_VALUE * n_VALUE;

        // Each workgroup/block has 8x16 = 128 threads.
        const THREADS_X_VALUE = 8;
        const THREADS_Y_VALUE = 16;

        // Instead of computing one element per thread, each thread computes a tile of outputs (2x4 = 8 elements):
        const THREAD_TILE_X_VALUE = 2;
        const THREAD_TILE_Y_VALUE = 4;

        // So one block computes a 16x64 output tile.
        const THREADS_X_DIM = THREADS_X_VALUE * THREAD_TILE_X_VALUE; // 16
        const THREADS_Y_DIM = THREADS_Y_VALUE * THREAD_TILE_Y_VALUE; // 64

        // Normally, a warp is 32 threads arranged in 1D. But you reshape them into a 2D layout:
        const WARP_SIZE = 32; // assume NVIDIA,
        const WARP_SHAPE_X_VALUE = 4;
        const WARP_SHAPE_Y_VALUE = WARP_SIZE / WARP_SHAPE_X_VALUE; // 8


        // This is a tunable parameter, which affect the shared memory usage.
        const TUNE_SPLIT_VALUE = 32;
        const A_SHARED_SIZE = THREADS_Y_DIM * TUNE_SPLIT_VALUE; // 2048
        const B_SHARED_SIZE = TUNE_SPLIT_VALUE * THREADS_X_DIM; // 512



        async function run(device) {

            // console.assert(WORKGROUP_SIZE_X % 64 === 0, "WORKGROUP_SIZE_X must be a multiple of 64");
            log("THREADS_X_VALUE: " + THREADS_X_VALUE);
            log("THREADS_Y_VALUE: " + THREADS_Y_VALUE);
            log("THREAD_TILE_X_VALUE: " + THREAD_TILE_X_VALUE);
            log("THREAD_TILE_Y_VALUE: " + THREAD_TILE_Y_VALUE);
            log("THREADS_X_DIM: " + THREADS_X_DIM);
            log("THREADS_Y_DIM: " + THREADS_Y_DIM);
            log("WARP_SIZE: " + WARP_SIZE);
            log("WARP_SHAPE_X_VALUE: " + WARP_SHAPE_X_VALUE);
            log("WARP_SHAPE_Y_VALUE: " + WARP_SHAPE_Y_VALUE);
            log("TUNE_SPLIT_VALUE: " + TUNE_SPLIT_VALUE);
            log("A_SHARED_SIZE: " + A_SHARED_SIZE);
            log("B_SHARED_SIZE: " + B_SHARED_SIZE);

            const computeShader = `
enable chromium_experimental_subgroup_matrix;
enable f16;

// ==== Tunables (filled by host) ====
const THREADS_X_VALUE       : u32 = ${THREADS_X_VALUE}u;
const THREADS_Y_VALUE       : u32 = ${THREADS_Y_VALUE}u;
const THREAD_TILE_X_VALUE   : u32 = ${THREAD_TILE_X_VALUE}u;
const THREAD_TILE_Y_VALUE   : u32 = ${THREAD_TILE_Y_VALUE}u;
const THREADS_X_DIM         : u32 = ${THREADS_X_DIM}u;       // = THREADS_X_VALUE * THREAD_TILE_X_VALUE
const THREADS_Y_DIM         : u32 = ${THREADS_Y_DIM}u;       // = THREADS_Y_VALUE * THREAD_TILE_Y_VALUE

const WARP_SIZE             : u32 = ${WARP_SIZE}u;           // typically 32 but queryable via @builtin(subgroup_size)
const WARP_SHAPE_X_VALUE    : u32 = ${WARP_SHAPE_X_VALUE}u;
const WARP_SHAPE_Y_VALUE    : u32 = ${WARP_SHAPE_Y_VALUE}u;  // usually = WARP_SIZE / WARP_SHAPE_X_VALUE

const TUNE_SPLIT_VALUE      : u32 = ${TUNE_SPLIT_VALUE}u;
const A_SHARED_SIZE         : u32 = ${A_SHARED_SIZE}u;       // = THREADS_Y_DIM * TUNE_SPLIT_VALUE
const B_SHARED_SIZE         : u32 = ${B_SHARED_SIZE}u;       // = TUNE_SPLIT_VALUE * THREADS_X_DIM

// === new base ids theme ===
fn get_local_id_x(local_id: vec3<u32>) -> u32 {
  // CUDA macro reconstructed from warp/lane math == local_id.x here
  return local_id.x;
}

fn get_local_id_y(local_id: vec3<u32>) -> u32 {
  return local_id.y;
}

fn get_block_id_x(wg_id: vec3<u32>) -> u32 {
  return wg_id.x;
}

fn get_block_id_y(wg_id: vec3<u32>) -> u32 {
  return wg_id.y;
}

fn get_global_id_x(wg_id: vec3<u32>, local_id: vec3<u32>) -> u32 {
  // (blockIdx.x * THREADS_X_VALUE + local_id.x)
  return get_block_id_x(wg_id) * THREADS_X_VALUE + get_local_id_x(local_id);
}

fn get_global_id_y(wg_id: vec3<u32>, local_id: vec3<u32>) -> u32 {
  return get_block_id_y(wg_id) * THREADS_Y_VALUE + get_local_id_y(local_id);
}

fn get_flattened_dim() -> u32 {
  return THREADS_X_VALUE * THREADS_Y_VALUE;
}

fn get_flattened_id(local_id: vec3<u32>) -> u32 {
  // CUDA used threadIdx.x; here use local_index flattened in workgroup
  // and ensure @workgroup_size(THREADS_X_VALUE, THREADS_Y_VALUE, 1)
  return local_id.y * THREADS_X_VALUE + local_id.x;
}

// === gridDim theme ===
// In WGSL you don't create a dim3; you *dispatch* (#workgroups).
// This helper mirrors the macro’s arithmetic so you can compute it
// the same way on the CPU side or in-shader if needed.
fn mk_gridDim(n: u32) -> vec2<u32> {
  return vec2<u32>(n / THREADS_X_DIM, n / THREADS_Y_DIM);
}

// === computed-per-thread dimensions theme ===
// fn THREADS_X_DIM_fn() -> u32 { return THREADS_X_VALUE * THREAD_TILE_X_VALUE; }
// fn THREADS_Y_DIM_fn() -> u32 { return THREADS_Y_VALUE * THREAD_TILE_Y_VALUE; }

// Keep names identical to macros (aliases):
// const THREADS_X_DIM : u32 = THREADS_X_VALUE * THREAD_TILE_X_VALUE;
// const THREADS_Y_DIM : u32 = THREADS_Y_VALUE * THREAD_TILE_Y_VALUE;

// === indexing macros theme ===
fn index2D(i: u32, j: u32, n: u32) -> u32 {
  return i * n + j;
}

// === block tiling theme ===
fn get_global_block_tile_y(wg_id: vec3<u32>) -> u32 {
  return get_block_id_y(wg_id) * THREADS_Y_DIM;
}
fn get_global_block_tile_x(wg_id: vec3<u32>) -> u32 {
  return get_block_id_x(wg_id) * THREADS_X_DIM;
}

// === dynamic shared memory theme ===
// (Sizes as values; allocation of var<workgroup> needs const sizes in WGSL)
// fn A_SHARED_SIZE_fn() -> u32 { return THREADS_Y_DIM * TUNE_SPLIT_VALUE; }
// fn B_SHARED_SIZE_fn() -> u32 { return TUNE_SPLIT_VALUE * THREADS_X_DIM; }
// const A_SHARED_SIZE : u32 = THREADS_Y_DIM * TUNE_SPLIT_VALUE;
// const B_SHARED_SIZE : u32 = TUNE_SPLIT_VALUE * THREADS_X_DIM;

// === warp reshaping theme ===
// WGSL "warp" ≈ subgroup; for 1:1 with your CUDA macros we use 32.
// const WARP_SIZE : u32 = 32u;

// fn WARP_SHAPE_Y_VALUE_fn() -> u32 { return WARP_SIZE / WARP_SHAPE_X_VALUE; }
// const WARP_SHAPE_Y_VALUE : u32 = WARP_SIZE / WARP_SHAPE_X_VALUE;

fn get_lane_id(local_id: vec3<u32>) -> u32 {
  // emulate lane within 32-wide warp using local_index
  return local_id.x % WARP_SIZE;
}
fn get_lane_id_x(local_id: vec3<u32>) -> u32 {
  return get_lane_id(local_id) % WARP_SHAPE_X_VALUE;
}
fn get_lane_id_y(local_id: vec3<u32>) -> u32 {
  return get_lane_id(local_id) / WARP_SHAPE_X_VALUE;
}
fn get_warp_id(local_id: vec3<u32>) -> u32 {
  return local_id.x / WARP_SIZE;
}
fn get_warp_id_x(local_id: vec3<u32>) -> u32 {
  return get_warp_id(local_id) % (THREADS_X_VALUE / WARP_SHAPE_X_VALUE);
}
fn get_warp_id_y(local_id: vec3<u32>) -> u32 {
  return get_warp_id(local_id) / (THREADS_X_VALUE / WARP_SHAPE_X_VALUE);
}
fn get_local_warp_tile_x(local_id: vec3<u32>) -> u32 {
  return get_warp_id_x(local_id) * WARP_SHAPE_X_VALUE * THREAD_TILE_X_VALUE;
}
fn get_local_warp_tile_y(local_id: vec3<u32>) -> u32 {
  return get_warp_id_y(local_id) * WARP_SHAPE_Y_VALUE * THREAD_TILE_Y_VALUE;
}
fn get_global_warp_tile_x(wg_id: vec3<u32>, local_id: vec3<u32>) -> u32 {
  return get_global_block_tile_x(wg_id) + get_local_warp_tile_x(local_id);
}
fn get_global_warp_tile_y(wg_id: vec3<u32>, local_id: vec3<u32>) -> u32 {
  return get_global_block_tile_y(wg_id) + get_local_warp_tile_y(local_id);
}

// === thread tiling theme ===
fn get_local_id_x_tiled(local_id: vec3<u32>, thread_tile_x: u32) -> u32 {
  return get_local_id_x(local_id) * THREAD_TILE_X_VALUE + thread_tile_x;
}
fn get_global_id_x_tiled(wg_id: vec3<u32>, local_id: vec3<u32>, thread_tile_x: u32) -> u32 {
  return get_global_block_tile_x(wg_id) + get_local_id_x_tiled(local_id, thread_tile_x);
}
fn get_local_id_y_tiled(local_id: vec3<u32>, thread_tile_y: u32) -> u32 {
  return get_local_id_y(local_id) * THREAD_TILE_Y_VALUE + thread_tile_y;
}
fn get_global_id_y_tiled(b: Builtins, thread_tile_y: u32) -> u32 {
  return get_global_block_tile_y(b) + get_local_id_y_tiled(b, thread_tile_y);
}



                @group(0) @binding(0) var<storage, read>       A : array<f16>;
                @group(0) @binding(1) var<storage, read>       B : array<f16>;
                @group(0) @binding(2) var<storage, read_write> C : array<f16>;


                @compute @workgroup_size(${THREADS_X_VALUE}, ${THREADS_Y_VALUE}, 1)
                fn main(
                  @builtin(workgroup_id)        wg_id   : vec3<u32>,
                  @builtin(local_invocation_id) local_id: vec3<u32>,
                ) {
                    var tmp = A[0];
                    var tmp2 = B[0];
                    C[0] = f16(1);
                }
            `;

            log(computeShader);

            const A = new Float16Array(A_LEN);
            const B = new Float16Array(B_LEN);
            const C = new Float16Array(C_LEN);

            for (let i = 0; i < A_LEN; i++) {
                A[i] = i;
            }

            // Fill B as an identity matrix (size MATRIX_K x MATRIX_N)
            for (let row = 0; row < n_VALUE; row++) {
                for (let col = 0; col < n_VALUE; col++) {
                    B[row * n_VALUE + col] = (row === col) ? 1.0 : 0.0;
                }
            }

            for (let i = 0; i < C_LEN; i++) {
                C[i] = 0.0;
            }


            const A_Buffer = device.createBuffer({
                size: A.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });

            const B_Buffer = device.createBuffer({
                size: B.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });

            const C_Buffer = device.createBuffer({
                size: C.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
            });

            // Create staging buffer for reading results
            const stagingBuffer = device.createBuffer({
                size: C.byteLength,
                usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
            });

            device.queue.writeBuffer(A_Buffer, 0, A);
            device.queue.writeBuffer(B_Buffer, 0, B);

            // Create compute pipeline
            const computePipeline = device.createComputePipeline({
                layout: "auto",
                compute: {
                    module: device.createShaderModule({
                        code: computeShader,
                    }),
                    entryPoint: "main",
                },
            });

            // Create bind group

            const bindGroup = device.createBindGroup({
                layout: computePipeline.getBindGroupLayout(0),
                entries: [
                    {
                        binding: 0,
                        resource: { buffer: A_Buffer },
                    },
                    {
                        binding: 1,
                        resource: { buffer: B_Buffer },
                    },
                    {
                        binding: 2,
                        resource: { buffer: C_Buffer },
                    },
                ],
            });

            // Create command encoder and compute pass
            const commandEncoder = device.createCommandEncoder();
            const computePass = commandEncoder.beginComputePass();
            computePass.setPipeline(computePipeline);
            computePass.setBindGroup(0, bindGroup);

            // Dispatch the compute shader

            const gridX = Math.ceil(n_VALUE / THREADS_X_VALUE);
            const gridY = Math.ceil(n_VALUE / THREADS_Y_VALUE);
            log("Dispatching gridX: " + gridX + ", gridY: " + gridY);

            computePass.dispatchWorkgroups(gridX, gridY, 1);
            computePass.end();

            // Copy output to staging buffer for reading
            commandEncoder.copyBufferToBuffer(
                C_Buffer,
                0,
                stagingBuffer,
                0,
                C.byteLength,
            );

            // Submit and wait for GPU execution
            const commandBuffer = commandEncoder.finish();
            device.queue.submit([commandBuffer]);

            let start = performance.now();

            await device.queue.onSubmittedWorkDone();

            let end = performance.now();

            log(
                "Compute shader executed successfully on GPU! " +
                (end - start) +
                "ms",
            );

            // Map and read the results
            await stagingBuffer.mapAsync(GPUMapMode.READ);
            const results = new Float16Array(stagingBuffer.getMappedRange());

            // Copy the data before unmapping to avoid detached ArrayBuffer
            const resultsCopy = new Float16Array(results);
            stagingBuffer.unmap();

            printMatrix(resultsCopy, n_VALUE, n_VALUE, false, 3);
        }

        async function main() {
            if (!navigator.gpu?.requestAdapter) {
                log("WebGPU is NOT available in this browser.");
                return;
            }

            const adapter = await navigator.gpu.requestAdapter();
            if (!adapter) {
                log("No GPU adapter found.");
                return;
            }

            log("Adapter found: " + (adapter.info.description || "Unnamed GPU"));

            const requiredFeatures = [
                "chromium-experimental-subgroup-matrix",
                "shader-f16",
            ];

            if (!requiredFeatures.every((f) => adapter.features.has(f))) {
                log("Missing required features:", requiredFeatures.join(", "));
                return;
            }

            const device = await adapter.requestDevice({ requiredFeatures });
            log("Running...");
            await run(device);
        }

        main();
    </script>
</body>

</html>