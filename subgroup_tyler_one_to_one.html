<!doctype html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <title>WebGPU Subgroup Matrix</title>
</head>

<body>
    <pre id="output"></pre>

    <script>
        function log(...args) {
            document.getElementById("output").textContent += args.join(" ") + "\n";
        }

        function printMatrix(A, rows, cols, verbose = false, padding = null) {
            // Helper to pad a value to a string of given width
            function padValue(val, width) {
                let s = String(val);
                if (width == null) return s;
                // If val is a number, format to fixed decimals for alignment
                if (typeof val === "number") {
                    // Try to keep 4 significant digits for floats
                    if (Number.isInteger(val)) {
                        s = val.toString();
                    } else {
                        s = val.toFixed(3);
                    }
                }
                // If it's an ellipsis, just center it
                if (s === "...") {
                    let left = Math.floor((width - 3) / 2);
                    let right = width - 3 - left;
                    return " ".repeat(left) + "..." + " ".repeat(right);
                }
                // Pad left for numbers, right for others
                return s.padStart(width, " ");
            }

            if (verbose) {
                // Print full matrix
                for (let i = 0; i < rows; i++) {
                    let row = [];
                    for (let j = 0; j < cols; j++) {
                        row.push(padValue(A[i * cols + j], padding));
                    }
                    log(row.join(" "));
                }
            } else {
                // Print abbreviated matrix with ellipsis
                for (let i = 0; i < rows; i++) {
                    if (i < 3 || i > rows - 4) {
                        let row = [];
                        for (let j = 0; j < cols; j++) {
                            if (j < 3 || j > cols - 4) {
                                row.push(padValue(A[i * cols + j], padding));
                            } else if (j === 3) {
                                row.push(padValue("...", padding));
                            }
                        }
                        log(row.join(" "));
                    } else if (i === 3) {
                        // Print a row of ellipsis, padded if needed
                        let ellipsisRow = [];
                        for (let j = 0; j < cols; j++) {
                            if (j < 3 || j > cols - 4) {
                                ellipsisRow.push(padValue("...", padding));
                            } else if (j === 3) {
                                ellipsisRow.push(padValue("...", padding));
                            }
                        }
                        log(ellipsisRow.join(" "));
                    }
                }
            }
        }

        // Problem size
        const n_VALUE = 2048;
        const A_LEN = n_VALUE * n_VALUE;
        const B_LEN = n_VALUE * n_VALUE;
        const C_LEN = n_VALUE * n_VALUE;

        // Shader constants (moved from override to JavaScript)
        // Logical workgroup "thread dims" (conceptual 2D layout of a 1D block)
        const THREADS_X_VALUE = 8;
        const THREADS_Y_VALUE = 16;

        // K tile (outer) loaded into shared memory each iteration
        const TUNE_SPLIT_VALUE = 32;

        // Warp reshape (lane grid inside a warp)
        // For NVIDIA, Intel GPU, warp is 32
        // MacOS is 64
        const WARP_SIZE = 32;
        const WARP_SHAPE_X_VALUE = 4;
        const WARP_SHAPE_Y_VALUE = WARP_SIZE / WARP_SHAPE_X_VALUE;

        // 
        const THREAD_TILE_X_VALUE = 2;
        const THREAD_TILE_Y_VALUE = 4;

        // Per-warp fragment replication (was not used in Tyler's code)
        // const WARP_TILE_X_VALUE = 8;
        // const WARP_TILE_Y_VALUE = 4;

        // Subgroup (cooperative) matrix base tile used by the WGSL intrinsics
        // According the specs:
        // 
        // subgroup_matrix_left<T, K, M> --- M rows x K cols
        // subgroup_matrix_right<T, N, K> --- K rows x N cols  
        // subgroup_matrix_result<T, N, M> --- N rows x M cols
        // 
        // result (M, N) = A (M, K) * B (K, N)        
        //
        const TENSOR_M = 8;
        const TENSOR_N = 16;
        const TENSOR_K = 16;

        // Derived per-workgroup output coverage (block tile size)
        const THREADS_X_DIM = THREADS_X_VALUE * THREAD_TILE_X_VALUE; // 8*2 = 16
        const THREADS_Y_DIM = THREADS_Y_VALUE * THREAD_TILE_Y_VALUE; // 16*4 = 64

        // THREADS_X_DIM must be multiple of TENSOR_N
        // THREADS_Y_DIM must be multiple of TENSOR_M
        // TUNE_SPLIT_VALUE must >= TENSOR_K

        // WARP_SHAPE_X_VALUE * THREAD_TILE_X_VALUE == TENSOR_N
        // WARP_SHAPE_Y_VALUE * THREAD_TILE_Y_VALUE == TENSOR_M

        // WARP_SHAPE_Y_VALUE > THREADS_Y_VALUE
        // WARP_SHAPE_X_VALUE > THREADS_X_VALUE

        // #define mk_gridDim(variable_name)                                              \
        // dim3 variable_name(n / THREADS_X_DIM, n / THREADS_Y_DIM)
        class GridDim {
            constructor(x, y) {
                this.x = x;
                this.y = y;
            }
        }

        function mk_gridDim(n) {
            return new GridDim(n / THREADS_X_DIM, n / THREADS_Y_DIM);
        }

        // Shared-memory (workgroup) tile sizes in *elements of f16*
        const A_SHARED_SIZE = THREADS_Y_DIM * TUNE_SPLIT_VALUE; // 64*32 = 2048
        const B_SHARED_SIZE = TUNE_SPLIT_VALUE * THREADS_X_DIM; // 32*16 = 512

        async function run(device) {

            if (WARP_SHAPE_Y_VALUE > THREADS_Y_VALUE) {
                log("WARP_RESHAPE_ERROR: WARP_SHAPE_Y_VALUE cannot be greater than THREADS_Y_VALUE");
                return;
            }
            if (WARP_SHAPE_X_VALUE > THREADS_X_VALUE) {
                log("WARP_RESHAPE_ERROR: WARP_SHAPE_X_VALUE cannot be greater than THREADS_X_VALUE");
                return;
            }

            function tensorCoreShapeCheck() {
                // Helper for error reporting
                function fail(msg) {
                    log("TENSOR_CORE_SHAPE_ERROR: " + msg);
                    throw new Error("TENSOR_CORE_SHAPE_ERROR: " + msg);
                }

                // List of supported shapes (M, N, K)
                const configs = [
                    { M: 16, N: 16, K: 16 },
                    { M: 32, N: 8, K: 16 },
                    { M: 8, N: 32, K: 16 },
                    // { M: 8, N: 8, K: 8 },
                ];

                let matched = false;
                for (const cfg of configs) {
                    if (
                        (THREADS_X_DIM % cfg.N === 0) &&
                        (THREADS_Y_DIM % cfg.M === 0) &&
                        (TUNE_SPLIT_VALUE >= cfg.K) &&
                        (WARP_SHAPE_X_VALUE * THREAD_TILE_X_VALUE === cfg.N) &&
                        (WARP_SHAPE_Y_VALUE * THREAD_TILE_Y_VALUE === cfg.M)
                    ) {
                        matched = true;
                        break;
                    }
                }
                if (!matched) {
                    fail("Tuning parameters could not be shaped into a valid tensor core configuration");
                }
            }

            tensorCoreShapeCheck();


            const computeShader = `
                diagnostic(off, chromium.subgroup_matrix_uniformity);
                
                enable chromium_experimental_subgroup_matrix;
                // enable subgroups;
                enable f16;

                @group(0) @binding(0) var<storage, read>       A : array<f16>;
                @group(0) @binding(1) var<storage, read>       B : array<f16>;
                @group(0) @binding(2) var<storage, read_write> C : array<f16>;

                
// ---------------- Base IDs theme ----------------
fn get_local_id_x(local_id: vec3<u32>) -> u32 { return (get_warp_id_x(local_id) * ${WARP_SHAPE_X_VALUE}) + get_lane_id_x(local_id); }
fn get_local_id_y(local_id: vec3<u32>) -> u32 { return (get_warp_id_y(local_id) * ${WARP_SHAPE_Y_VALUE}) + get_lane_id_y(local_id); }

fn get_block_id_x(w_id: vec3<u32>) -> u32 { return w_id.x; }
fn get_block_id_y(w_id: vec3<u32>) -> u32 { return w_id.y; }

fn get_global_id_x(local_id: vec3<u32>, w_id: vec3<u32>) -> u32 { return (get_block_id_x(w_id) * ${THREADS_X_VALUE}) + get_local_id_x(local_id); }
fn get_global_id_y(local_id: vec3<u32>, w_id: vec3<u32>) -> u32 { return (get_block_id_y(w_id) * ${THREADS_Y_VALUE}) + get_local_id_y(local_id); }

fn get_flattened_dim() -> u32 { return ${THREADS_X_VALUE} * ${THREADS_Y_VALUE}; }
fn get_flattened_id(local_id: vec3<u32>) -> u32 { return local_id.x; }


// ---------------- Indexing ----------------
fn index2D(i: u32, j: u32, n: u32) -> u32 { return i * n + j; }

// ---------------- Block tiling ----------------
fn get_global_block_tile_y(w_id: vec3<u32>) -> u32 { return get_block_id_y(w_id) * ${THREADS_Y_DIM}; }
fn get_global_block_tile_x(w_id: vec3<u32>) -> u32 { return get_block_id_x(w_id) * ${THREADS_X_DIM}; }

// ---------------- Warp reshaping ----------------
fn get_lane_id(local_id: vec3<u32>) -> u32 { return local_id.x % ${WARP_SIZE}; }
fn get_lane_id_x(local_id: vec3<u32>) -> u32 { return get_lane_id(local_id) % ${WARP_SHAPE_X_VALUE}; }
fn get_lane_id_y(local_id: vec3<u32>) -> u32 { return get_lane_id(local_id) / ${WARP_SHAPE_X_VALUE}; }

fn get_warp_id(local_id: vec3<u32>) -> u32 { return local_id.x / ${WARP_SIZE}; }
fn get_warp_id_x(local_id: vec3<u32>) -> u32 { return get_warp_id(local_id) % (${THREADS_X_VALUE} / ${WARP_SHAPE_X_VALUE}); }
fn get_warp_id_y(local_id: vec3<u32>) -> u32 { return get_warp_id(local_id) / (${THREADS_X_VALUE} / ${WARP_SHAPE_X_VALUE}); }

fn get_local_warp_tile_x(local_id: vec3<u32>) -> u32 { return get_warp_id_x(local_id) * ${WARP_SHAPE_X_VALUE} * ${THREAD_TILE_X_VALUE}; }
fn get_local_warp_tile_y(local_id: vec3<u32>) -> u32 { return get_warp_id_y(local_id) * ${WARP_SHAPE_Y_VALUE} * ${THREAD_TILE_Y_VALUE}; }

fn get_global_warp_tile_x(local_id: vec3<u32>, w_id: vec3<u32>) -> u32 { return get_global_block_tile_x(w_id) + get_local_warp_tile_x(local_id); }
fn get_global_warp_tile_y(local_id: vec3<u32>, w_id: vec3<u32>) -> u32 { return get_global_block_tile_y(w_id) + get_local_warp_tile_y(local_id); }

// ---------------- Thread tiling ----------------
fn get_local_id_x_tiled(local_id: vec3<u32>, thread_tile_x: u32) -> u32 { return get_local_id_x(local_id) * ${THREAD_TILE_X_VALUE} + thread_tile_x; }
fn get_global_id_x_tiled(local_id: vec3<u32>, w_id: vec3<u32>, thread_tile_x: u32) -> u32 { return get_global_block_tile_x(w_id) + get_local_id_x_tiled(local_id, thread_tile_x); }
fn get_local_id_y_tiled(local_id: vec3<u32>, thread_tile_y: u32) -> u32 { return get_local_id_y(local_id) * ${THREAD_TILE_Y_VALUE} + thread_tile_y; }
fn get_global_id_y_tiled(local_id: vec3<u32>, w_id: vec3<u32>, thread_tile_y: u32) -> u32 { return get_global_block_tile_y(w_id) + get_local_id_y_tiled(local_id, thread_tile_y); }

    
    const kColMajor = false;
    

var<workgroup> A_shared : array<f16, ${A_SHARED_SIZE}>;
var<workgroup> B_shared : array<f16, ${B_SHARED_SIZE}>;


// Cooperative, strided load of one tile of A and B from global into workgroup.
// Reads/writes A_shared/B_shared directly (cannot pass workgroup arrays as params).
fn load_shared_memory_tile(k_outer: u32, n: u32, local_id: vec3<u32>, w_id: vec3<u32>) {
  let T        = get_flattened_dim();
  let tid      = get_flattened_id(local_id);

  // ---- A tile geometry & addressing ----
  let A_rows : u32         = ${THREADS_Y_DIM};
  let A_cols : u32         = ${TUNE_SPLIT_VALUE};
  let A_ld_shared : u32    = A_cols;                        // leading dim of A_shared
  let A_tile_row0 : u32    = get_global_block_tile_y(w_id); // global row origin of A tile
  let A_tile_col0 : u32    = k_outer;                       // global col origin of A tile

  // ---- B tile geometry & addressing ----
  let B_rows : u32         = ${TUNE_SPLIT_VALUE};
  let B_cols : u32         = ${THREADS_X_DIM};
  let B_ld_shared : u32    = B_cols;                        // leading dim of B_shared
  let B_tile_row0 : u32    = k_outer;                       // global row origin of B tile
  let B_tile_col0 : u32    = get_global_block_tile_x(w_id); // global col origin of B tile

  // ---- Cooperative copy: A (row-major logical) ----
  let A_elems = A_rows * A_cols;
  var e = tid;
  while (e < A_elems) {
    let i = e / A_cols;          // 0..A_rows-1
    let j = e % A_cols;          // 0..A_cols-1
    
    // Global index in A (source is NxN laid out row-major as array<f16>)
    let g_row = A_tile_row0 + i;
    let g_col = A_tile_col0 + j;
    let g_idx = index2D(g_row, g_col, n);

    // Shared index (with padding in leading dim)
    let s_idx = index2D(i, j, A_ld_shared);
    A_shared[s_idx] = A[g_idx];
    e += T;
  }

  // ---- Cooperative copy: B ----
  let B_elems = B_rows * B_cols;
  e = tid;
  while (e < B_elems) {
    let i = e / B_cols;          // 0..B_rows-1
    let j = e % B_cols;          // 0..B_cols-1
    
    // Global index in B
    let g_row = B_tile_row0 + i;
    let g_col = B_tile_col0 + j;
    let g_idx = index2D(g_row, g_col, n);

    // Shared index
    let s_idx = index2D(i, j, B_ld_shared);
    B_shared[s_idx] = B[g_idx];
    e += T;
  }

  workgroupBarrier(); // ensure tiles are resident before MMA
}



                // Change block dimensions to a 1D launch
                @compute @workgroup_size(${THREADS_X_VALUE} * ${THREADS_Y_VALUE}, 1, 1)
                fn main(
                    @builtin(local_invocation_id) local_id : vec3<u32>,
                    @builtin(workgroup_id) w_id : vec3<u32>,
                    @builtin(global_invocation_id) global_id : vec3<u32>,
                    @builtin(subgroup_id) s_id : u32,
                    // @builtin(subgroup_size) s_size : u32,
                ) {
                    let i: u32 = get_global_id_y(local_id, w_id);
                    let j: u32 = get_global_id_x(local_id, w_id);
                    let local_i: u32 = get_local_id_y(local_id);
                    let local_j: u32 = get_local_id_x(local_id);

                    var a_frag : subgroup_matrix_left<f16, ${TENSOR_K}, ${TENSOR_M}>;
                    var b_frag : subgroup_matrix_right<f16, ${TENSOR_N}, ${TENSOR_K}>;
                    var acc_frag : subgroup_matrix_result<f16, ${TENSOR_N}, ${TENSOR_M}>;

                    for (var k_outer: u32 = 0u; k_outer < ${n_VALUE}; k_outer = k_outer + ${TUNE_SPLIT_VALUE}) {
                    
                        load_shared_memory_tile(k_outer, ${n_VALUE}, local_id, w_id);
                        
                        for (var k_inner: u32 = 0u; k_inner < ${TUNE_SPLIT_VALUE}; k_inner = k_inner + ${TENSOR_K}) {

                            // Load A tile
                            let A_local_warp_tile = index2D(
                                get_local_warp_tile_y(local_id),    // row offset of this warp's tile in A_shared
                                k_inner,                            // column offset in A_shared
                                ${TUNE_SPLIT_VALUE}                 // stride of A_shared
                            );
                            a_frag = subgroupMatrixLoad<subgroup_matrix_left<f16, ${TENSOR_K}, ${TENSOR_M}>>(
                                &A_shared, 
                                A_local_warp_tile, 
                                kColMajor, 
                                ${TUNE_SPLIT_VALUE}
                            );
                            
                            // Load B tile
                            let B_local_warp_tile = index2D(
                                k_inner,
                                get_local_warp_tile_x(local_id),
                                ${THREADS_X_DIM}
                            );
                            b_frag = subgroupMatrixLoad<subgroup_matrix_right<f16, ${TENSOR_N}, ${TENSOR_K}>>(
                                &B_shared, 
                                B_local_warp_tile, 
                                kColMajor, 
                                ${THREADS_X_DIM}
                            );

                             // Perform the matrix multiply-accumulate on the fragments
                            acc_frag = subgroupMatrixMultiplyAccumulate(a_frag, b_frag, acc_frag);
                        }

                        // End of process shared memory tile phase
                        
                        workgroupBarrier();
                    }

                    // Write the result from the accumulator fragment back to global memory                    
                    let C_global_warp_tile = index2D(
                        get_global_warp_tile_y(local_id, w_id), // row offset of this warp's tile in C
                        get_global_warp_tile_x(local_id, w_id), // column offset of this warp's tile in C
                        ${n_VALUE}                              // global leading dimension of C
                    );
                    subgroupMatrixStore(&C, C_global_warp_tile, acc_frag, kColMajor, ${n_VALUE});

                }
            `;

            log(computeShader);

            const A = new Float16Array(A_LEN);
            const B = new Float16Array(B_LEN);
            const C = new Float16Array(C_LEN);

            for (let i = 0; i < A_LEN; i++) {
                A[i] = 1;
            }

            // Fill B as an identity matrix (size MATRIX_K x MATRIX_N)
            for (let row = 0; row < n_VALUE; row++) {
                for (let col = 0; col < n_VALUE; col++) {
                    B[row * n_VALUE + col] = (row === col) ? 1.0 : 0.0;
                }
            }

            for (let i = 0; i < C_LEN; i++) {
                C[i] = 0.0;
            }


            const A_Buffer = device.createBuffer({
                size: A.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });

            const B_Buffer = device.createBuffer({
                size: B.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_DST,
            });

            const C_Buffer = device.createBuffer({
                size: C.byteLength,
                usage: GPUBufferUsage.STORAGE | GPUBufferUsage.COPY_SRC,
            });

            // Create staging buffer for reading results
            const stagingBuffer = device.createBuffer({
                size: C.byteLength,
                usage: GPUBufferUsage.MAP_READ | GPUBufferUsage.COPY_DST,
            });

            device.queue.writeBuffer(A_Buffer, 0, A);
            device.queue.writeBuffer(B_Buffer, 0, B);

            // Create compute pipeline
            const computePipeline = device.createComputePipeline({
                layout: "auto",
                compute: {
                    module: device.createShaderModule({
                        code: computeShader,
                    }),
                    entryPoint: "main",
                },
            });

            // Create bind group

            const bindGroup = device.createBindGroup({
                layout: computePipeline.getBindGroupLayout(0),
                entries: [
                    {
                        binding: 0,
                        resource: { buffer: A_Buffer },
                    },
                    {
                        binding: 1,
                        resource: { buffer: B_Buffer },
                    },
                    {
                        binding: 2,
                        resource: { buffer: C_Buffer },
                    },
                ],
            });

            // Create command encoder and compute pass
            const commandEncoder = device.createCommandEncoder();
            const computePass = commandEncoder.beginComputePass();
            computePass.setPipeline(computePipeline);
            computePass.setBindGroup(0, bindGroup);

            // Dispatch the compute shader

            const gridX = mk_gridDim(n_VALUE).x;
            const gridY = mk_gridDim(n_VALUE).y;
            log("Dispatching gridX: " + gridX + ", gridY: " + gridY);

            computePass.dispatchWorkgroups(gridX, gridY, 1);
            computePass.end();

            // Copy output to staging buffer for reading
            commandEncoder.copyBufferToBuffer(
                C_Buffer,
                0,
                stagingBuffer,
                0,
                C.byteLength,
            );

            // Submit and wait for GPU execution
            const commandBuffer = commandEncoder.finish();
            device.queue.submit([commandBuffer]);

            let start = performance.now();

            await device.queue.onSubmittedWorkDone();

            let end = performance.now();

            log(
                "Compute shader executed successfully on GPU! " +
                (end - start) +
                "ms",
            );

            // Map and read the results
            await stagingBuffer.mapAsync(GPUMapMode.READ);
            const results = new Float16Array(stagingBuffer.getMappedRange());

            // Copy the data before unmapping to avoid detached ArrayBuffer
            const resultsCopy = new Float16Array(results);
            stagingBuffer.unmap();

            printMatrix(resultsCopy, n_VALUE, n_VALUE, false, 3);
        }

        async function main() {
            if (!navigator.gpu?.requestAdapter) {
                log("WebGPU is NOT available in this browser.");
                return;
            }

            const adapter = await navigator.gpu.requestAdapter();
            if (!adapter) {
                log("No GPU adapter found.");
                return;
            }

            log("Adapter found: " + (adapter.info.description || "Unnamed GPU"));

            const requiredFeatures = [
                "chromium-experimental-subgroup-matrix",
                "shader-f16",
                // "subgroups",
            ];

            if (!requiredFeatures.every((f) => adapter.features.has(f))) {
                log("Missing required features:", requiredFeatures.join(", "));
                return;
            }

            const device = await adapter.requestDevice({ requiredFeatures });
            log("Running...");
            await run(device);
        }

        main();
    </script>
</body>

</html>